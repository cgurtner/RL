{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njb_ProuHiOe"
      },
      "source": [
        "# Graded lab: Implement DQN for LunarLander\n",
        "\n",
        "This lab is a modified verstion of a notebookfrom the Deep RL Course on HuggingFace.\n",
        "\n",
        "In this notebook, you'll train your **Deep Q-Network (DQN) agent** to play an Atari game. Your agent controls a spaceship, the Lunar Lander, to learn how to **land correctly on the Moon**.\n",
        "\n",
        "*All your answers should be written in this notebook. You shouldnâ€™t need to write or modify any other files. The parts of code that need to be changed as labelled as TODOs in the comments. You should execute every block of code to not miss any dependency.*\n",
        "\n",
        "### The environment\n",
        "\n",
        "We will use the [LunarLander-v2](https://gymnasium.farama.org/environments/box2d/lunar_lander/) environment from Gymnasium. This environment is a classic rocket trajectory optimization problem. According to Pontryaginâ€™s maximum principle, it is optimal to fire the engine at full throttle or turn it off. This is the reason why this environment has discrete actions: engine on or off."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PF46MwbZD00b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<video controls autoplay><source src=\"https://huggingface.co/sb3/ppo-LunarLander-v2/resolve/main/replay.mp4\" type=\"video/mp4\"></video>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%%html\n",
        "<video controls autoplay><source src=\"https://huggingface.co/sb3/ppo-LunarLander-v2/resolve/main/replay.mp4\" type=\"video/mp4\"></video>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qq3GYyfDl8_"
      },
      "source": [
        "### Note on HuggingFace\n",
        "\n",
        "You can easily find the HuggingFace original notebook which uses the [Stable-Baselines3](https://stable-baselines3.readthedocs.io/en/master/). This library provides a set of reliable implementations of reinforcement learning algorithms in PyTorch.\n",
        "\n",
        "The Hugging Face Hub ðŸ¤— works as a central place where anyone can share and explore models and datasets. It has versioning, metrics, visualizations and other features that will allow you to easily collaborate with others.\n",
        "\n",
        "You can see here all the Deep reinforcement Learning models available here https://huggingface.co/models?pipeline_tag=reinforcement-learning&sort=downloads"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeDAH0h0EBiG"
      },
      "source": [
        "## Install dependencies and create a virtual screen ðŸ”½\n",
        "\n",
        "The first step is to install the dependencies, weâ€™ll install multiple ones.\n",
        "\n",
        "- `gymnasium[box2d]`: Contains the LunarLander-v2 environment\n",
        "- `stable-baselines3[extra]`: The deep reinforcement learning library.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "yQIGLPDkGhgG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "zsh:1: command not found: apt\n"
          ]
        }
      ],
      "source": [
        "!apt install swig cmake"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9XaULfDZDvrC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "zsh:1: no matches found: gymnasium[box2d]\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium[box2d]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "qsUd3XZxDl9D"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: stable-baselines3==2.0.0a5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (2.0.0a5)\n",
            "Requirement already satisfied: gymnasium==0.28.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from stable-baselines3==2.0.0a5) (0.28.1)\n",
            "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from stable-baselines3==2.0.0a5) (1.24.4)\n",
            "Requirement already satisfied: torch>=1.11 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from stable-baselines3==2.0.0a5) (2.2.2)\n",
            "Requirement already satisfied: cloudpickle in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from stable-baselines3==2.0.0a5) (3.1.0)\n",
            "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from stable-baselines3==2.0.0a5) (1.4.1)\n",
            "Requirement already satisfied: matplotlib in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from stable-baselines3==2.0.0a5) (3.8.0)\n",
            "Requirement already satisfied: jax-jumpy>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from gymnasium==0.28.1->stable-baselines3==2.0.0a5) (1.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from gymnasium==0.28.1->stable-baselines3==2.0.0a5) (4.8.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from gymnasium==0.28.1->stable-baselines3==2.0.0a5) (0.0.4)\n",
            "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch>=1.11->stable-baselines3==2.0.0a5) (3.13.1)\n",
            "Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch>=1.11->stable-baselines3==2.0.0a5) (1.11.1)\n",
            "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch>=1.11->stable-baselines3==2.0.0a5) (2.8.8)\n",
            "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch>=1.11->stable-baselines3==2.0.0a5) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch>=1.11->stable-baselines3==2.0.0a5) (2024.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->stable-baselines3==2.0.0a5) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->stable-baselines3==2.0.0a5) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->stable-baselines3==2.0.0a5) (4.29.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->stable-baselines3==2.0.0a5) (1.3.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->stable-baselines3==2.0.0a5) (21.3)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->stable-baselines3==2.0.0a5) (9.0.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->stable-baselines3==2.0.0a5) (3.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->stable-baselines3==2.0.0a5) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas->stable-baselines3==2.0.0a5) (2021.3)\n",
            "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3==2.0.0a5) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jinja2->torch>=1.11->stable-baselines3==2.0.0a5) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from sympy->torch>=1.11->stable-baselines3==2.0.0a5) (1.2.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install stable-baselines3==2.0.0a5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEKeXQJsQCYm"
      },
      "source": [
        "During the notebook, we'll need to generate a replay video. To do so, with colab, **we need to have a virtual screen to be able to render the environment** (and thus record the frames).\n",
        "\n",
        "Hence the following cell will install virtual screen libraries and create and run a virtual screen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5f2cGkdP-mb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Password:\n",
            "sudo: a password is required\n",
            "Password:"
          ]
        }
      ],
      "source": [
        "!sudo apt-get update\n",
        "!sudo apt-get install -y python3-opengl\n",
        "!apt install ffmpeg\n",
        "!apt install xvfb\n",
        "!pip3 install pyvirtualdisplay"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCwBTAwAW9JJ"
      },
      "source": [
        "To make sure the new installed libraries are used, **sometimes it's required to restart the notebook runtime**. The next cell will force the **runtime to crash, so you'll need to connect again and run the code starting from here**. Thanks to this trick, **we will be able to run our virtual screen.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYvkbef7XEMi"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BE5JWP5rQIKf"
      },
      "outputs": [],
      "source": [
        "# Virtual display\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "virtual_display = Display(visible=0, size=(1400, 900))\n",
        "virtual_display.start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrgpVFqyENVf"
      },
      "source": [
        "## Import the packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cygWLPGsEQ0m"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.monitor import Monitor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIrKGGSlENZB"
      },
      "source": [
        "## Create the LunarLander environment and understand how it works\n",
        "\n",
        "### [The environment](https://gymnasium.farama.org/environments/box2d/lunar_lander/)\n",
        "\n",
        "The goal is to train our agent, a [Lunar Lander](https://gymnasium.farama.org/environments/box2d/lunar_lander/), **to land correctly on the moon**. To do that, the agent needs to learn **to adapt its speed and position (horizontal, vertical, and angular) to land correctly.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNPG0g_UGCfh"
      },
      "outputs": [],
      "source": [
        "# We create our environment with gym.make(\"<name_of_the_environment>\")\n",
        "env = gym.make(\"LunarLander-v2\")\n",
        "env.reset()\n",
        "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
        "print(\"Observation Space Shape\", env.observation_space.shape)\n",
        "print(\"Sample observation\", env.observation_space.sample()) # Get a random observation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MXc15qFE0M9"
      },
      "source": [
        "We see with `Observation Space Shape (8,)` that the observation is a vector of size 8, where each value contains different information about the lander:\n",
        "- Horizontal pad coordinate (x)\n",
        "- Vertical pad coordinate (y)\n",
        "- Horizontal speed (x)\n",
        "- Vertical speed (y)\n",
        "- Angle\n",
        "- Angular speed\n",
        "- If the left leg contact point has touched the land (boolean)\n",
        "- If the right leg contact point has touched the land (boolean)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "We5WqOBGLoSm"
      },
      "outputs": [],
      "source": [
        "print(\"\\n _____ACTION SPACE_____ \\n\")\n",
        "print(\"Action Space Shape\", env.action_space.n)\n",
        "print(\"Action Space Sample\", env.action_space.sample()) # Take a random action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyxXwkI2Magx"
      },
      "source": [
        "The action space (the set of possible actions the agent can take) is discrete with 4 actions available:\n",
        "\n",
        "- Action 0: Do nothing,\n",
        "- Action 1: Fire left orientation engine,\n",
        "- Action 2: Fire the main engine,\n",
        "- Action 3: Fire right orientation engine.\n",
        "\n",
        "Reward function (the function that will gives a reward at each timestep):\n",
        "\n",
        "After every step a reward is granted. The total reward of an episode is the **sum of the rewards for all the steps within that episode**.\n",
        "\n",
        "For each step, the reward:\n",
        "\n",
        "- Is increased/decreased the closer/further the lander is to the landing pad.\n",
        "-  Is increased/decreased the slower/faster the lander is moving.\n",
        "- Is decreased the more the lander is tilted (angle not horizontal).\n",
        "- Is increased by 10 points for each leg that is in contact with the ground.\n",
        "- Is decreased by 0.03 points each frame a side engine is firing.\n",
        "- Is decreased by 0.3 points each frame the main engine is firing.\n",
        "\n",
        "The episode receive an **additional reward of -100 or +100 points for crashing or landing safely respectively.**\n",
        "\n",
        "An episode is **considered a solution if it scores at least 200 points.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JF9Wm56GDl9K"
      },
      "source": [
        "#### Vectorized Environment\n",
        "\n",
        "- We create a vectorized environment (a method for stacking multiple independent environments into a single environment) of 16 environments, this way, **we'll have more diverse experiences during the training.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9D0ni6wVDl9L"
      },
      "outputs": [],
      "source": [
        "# Create the environment\n",
        "env = make_vec_env('LunarLander-v2', n_envs=16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgrE86r5E5IK"
      },
      "source": [
        "## Create the Model\n",
        "\n",
        "Remember the goal: **being able to land the Lunar Lander to the Landing Pad correctly by controlling left, right and main orientation engine**. Based on this, s build the algorithm we're going to use to solve this Problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HV4yiUM_9_Ka"
      },
      "source": [
        "To solve this problem, you're going to implement DQN from scratch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nxI6hT1GE4-A"
      },
      "outputs": [],
      "source": [
        "#### TODO: Define your DQN agent from scratch!\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import deque\n",
        "\n",
        "#####################################\n",
        "# Q-Network Definition\n",
        "#####################################\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, hidden_dim=64):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, action_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "#####################################\n",
        "# Replay Buffer\n",
        "#####################################\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, buffer_size=100000):\n",
        "        self.buffer = deque(maxlen=buffer_size)\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size=64):\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "\n",
        "        return (np.array(states, dtype=np.float32),\n",
        "                np.array(actions, dtype=np.int64),\n",
        "                np.array(rewards, dtype=np.float32),\n",
        "                np.array(next_states, dtype=np.float32),\n",
        "                np.array(dones, dtype=np.float32))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "#####################################\n",
        "# DQN Agent\n",
        "#####################################\n",
        "class DQNAgent:\n",
        "    def __init__(\n",
        "        self,\n",
        "        state_dim,\n",
        "        action_dim,\n",
        "        gamma=0.99,\n",
        "        lr=1e-3,\n",
        "        batch_size=64,\n",
        "        buffer_size=100000,\n",
        "        min_buffer_size=1000,\n",
        "        epsilon_start=1.0,\n",
        "        epsilon_end=0.01,\n",
        "        epsilon_decay=50000,  # Steps over which epsilon decays\n",
        "        tau=1e-3,  # For soft update of target network\n",
        "        device='cpu'\n",
        "    ):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.gamma = gamma\n",
        "        self.lr = lr\n",
        "        self.batch_size = batch_size\n",
        "        self.min_buffer_size = min_buffer_size\n",
        "        self.epsilon_start = epsilon_start\n",
        "        self.epsilon_end = epsilon_end\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.tau = tau\n",
        "        self.device = device\n",
        "\n",
        "        self.q_network = QNetwork(state_dim, action_dim).to(device)\n",
        "        self.target_network = QNetwork(state_dim, action_dim).to(device)\n",
        "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "\n",
        "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=self.lr)\n",
        "        self.replay_buffer = ReplayBuffer(buffer_size)\n",
        "\n",
        "        self.epsilon = epsilon_start\n",
        "        self.global_step = 0\n",
        "\n",
        "    def select_action(self, state):\n",
        "        # Epsilon-greedy policy\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            return np.random.randint(self.action_dim)\n",
        "        else:\n",
        "            state_t = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
        "            with torch.no_grad():\n",
        "                q_values = self.q_network(state_t)\n",
        "            return q_values.argmax(dim=1).item()\n",
        "\n",
        "    def store_transition(self, state, action, reward, next_state, done):\n",
        "        self.replay_buffer.add(state, action, reward, next_state, done)\n",
        "\n",
        "    def update_epsilon(self):\n",
        "        # Linear decay of epsilon\n",
        "        self.epsilon = max(self.epsilon_end, self.epsilon - (self.epsilon_start - self.epsilon_end) / self.epsilon_decay)\n",
        "\n",
        "    def soft_update(self):\n",
        "        # Soft update target network parameters\n",
        "        for target_param, local_param in zip(self.target_network.parameters(), self.q_network.parameters()):\n",
        "            target_param.data.copy_(self.tau * local_param.data + (1.0 - self.tau) * target_param.data)\n",
        "\n",
        "    def train_step(self):\n",
        "        if len(self.replay_buffer) < self.min_buffer_size:\n",
        "            return\n",
        "\n",
        "        # Sample from replay buffer\n",
        "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)\n",
        "\n",
        "        states_t = torch.FloatTensor(states).to(self.device)\n",
        "        actions_t = torch.LongTensor(actions).to(self.device).unsqueeze(1)\n",
        "        rewards_t = torch.FloatTensor(rewards).to(self.device).unsqueeze(1)\n",
        "        next_states_t = torch.FloatTensor(next_states).to(self.device)\n",
        "        dones_t = torch.FloatTensor(dones).to(self.device).unsqueeze(1)\n",
        "\n",
        "        # Compute current Q values\n",
        "        q_values = self.q_network(states_t).gather(1, actions_t)\n",
        "\n",
        "        # Compute next Q values from target network\n",
        "        with torch.no_grad():\n",
        "            next_q_values = self.target_network(next_states_t).max(dim=1, keepdim=True)[0]\n",
        "\n",
        "        # Compute the target Q values\n",
        "        target_q_values = rewards_t + (self.gamma * next_q_values * (1 - dones_t))\n",
        "\n",
        "        # Compute loss\n",
        "        loss = nn.MSELoss()(q_values, target_q_values)\n",
        "\n",
        "        # Gradient descent\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # Update target network\n",
        "        self.soft_update()\n",
        "\n",
        "        # Update epsilon\n",
        "        self.update_epsilon()\n",
        "\n",
        "        self.global_step += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optimization of DQN Agent\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'DQNAgent' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 64>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m q_values\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m#####################################\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Update DQN Agent for Double DQN\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m#####################################\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mDQNAgentImproved\u001b[39;00m(DQNAgent):\n\u001b[1;32m     65\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m    Updated DQN agent to use Double DQN and Dueling Q-Network.\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, dueling\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n",
            "\u001b[0;31mNameError\u001b[0m: name 'DQNAgent' is not defined"
          ]
        }
      ],
      "source": [
        "# Reward shaping, Double DQN, and Dueling DQN Optimizations\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import deque\n",
        "\n",
        "#####################################\n",
        "# Reward Shaping for Smooth Landings\n",
        "#####################################\n",
        "def reward_shaping(reward, state, next_state):\n",
        "    \"\"\"\n",
        "    Add custom penalties or rewards to encourage smooth landings:\n",
        "    - Penalize sharp angle changes (angular velocity).\n",
        "    - Penalize horizontal drift (horizontal speed).\n",
        "    \"\"\"\n",
        "    angle_penalty = -abs(next_state[4]) * 0.5  # Penalize large angles\n",
        "    horizontal_speed_penalty = -abs(next_state[2]) * 0.3  # Penalize horizontal drift\n",
        "    reward += angle_penalty + horizontal_speed_penalty\n",
        "    return reward\n",
        "\n",
        "#####################################\n",
        "# Dueling Q-Network\n",
        "#####################################\n",
        "class DuelingQNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    Dueling architecture to separate state value and action advantage.\n",
        "    \"\"\"\n",
        "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
        "        super(DuelingQNetwork, self).__init__()\n",
        "        self.feature_layer = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        \n",
        "        # State value stream\n",
        "        self.value_stream = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "        \n",
        "        # Advantage stream\n",
        "        self.advantage_stream = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, action_dim)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        features = self.feature_layer(x)\n",
        "        values = self.value_stream(features)\n",
        "        advantages = self.advantage_stream(features)\n",
        "        \n",
        "        # Combine value and advantage streams\n",
        "        q_values = values + (advantages - advantages.mean())\n",
        "        return q_values\n",
        "\n",
        "#####################################\n",
        "# Update DQN Agent for Double DQN\n",
        "#####################################\n",
        "class DQNAgentImproved(DQNAgent):\n",
        "    \"\"\"\n",
        "    Updated DQN agent to use Double DQN and Dueling Q-Network.\n",
        "    \"\"\"\n",
        "    def __init__(self, *args, dueling=True, **kwargs):\n",
        "        super(DQNAgentImproved, self).__init__(*args, **kwargs)\n",
        "        # Replace Q-Network with Dueling Q-Network if specified\n",
        "        if dueling:\n",
        "            self.q_network = DuelingQNetwork(self.state_dim, self.action_dim).to(self.device)\n",
        "            self.target_network = DuelingQNetwork(self.state_dim, self.action_dim).to(self.device)\n",
        "            self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "    \n",
        "    def train_step(self):\n",
        "        \"\"\"\n",
        "        Update using Double DQN to reduce overestimation bias.\n",
        "        \"\"\"\n",
        "        if len(self.replay_buffer) < self.min_buffer_size:\n",
        "            return\n",
        "\n",
        "        # Sample from replay buffer\n",
        "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)\n",
        "\n",
        "        states_t = torch.FloatTensor(states).to(self.device)\n",
        "        actions_t = torch.LongTensor(actions).to(self.device).unsqueeze(1)\n",
        "        rewards_t = torch.FloatTensor(rewards).to(self.device).unsqueeze(1)\n",
        "        next_states_t = torch.FloatTensor(next_states).to(self.device)\n",
        "        dones_t = torch.FloatTensor(dones).to(self.device).unsqueeze(1)\n",
        "\n",
        "        # Current Q-values\n",
        "        q_values = self.q_network(states_t).gather(1, actions_t)\n",
        "\n",
        "        # Double DQN Target Q-values\n",
        "        with torch.no_grad():\n",
        "            next_actions = self.q_network(next_states_t).argmax(dim=1, keepdim=True)\n",
        "            next_q_values = self.target_network(next_states_t).gather(1, next_actions)\n",
        "\n",
        "        target_q_values = rewards_t + self.gamma * next_q_values * (1 - dones_t)\n",
        "\n",
        "        # Loss calculation\n",
        "        loss = nn.MSELoss()(q_values, target_q_values)\n",
        "\n",
        "        # Gradient descent\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # Update target network\n",
        "        self.soft_update()\n",
        "\n",
        "        # Update epsilon\n",
        "        self.update_epsilon()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClJJk88yoBUi"
      },
      "source": [
        "## Train the DQN agent\n",
        "- Let's train our agent for 1,000,000 timesteps, don't forget to use GPU (on your local installation, Google Colab or similar). You will notice that experiments will take considerably longer than previous labs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bQzQ-QcE3zo"
      },
      "source": [
        "#### Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "poBCy9u_csyR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import trange\n",
        "\n",
        "# Use GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Initialize the agent with parameters (these are examples, feel free to adjust)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.n\n",
        "agent = DQNAgentImproved(\n",
        "    state_dim=state_dim,\n",
        "    action_dim=action_dim,\n",
        "    gamma=0.99,\n",
        "    lr=1e-3,\n",
        "    batch_size=64,\n",
        "    buffer_size=100000,\n",
        "    min_buffer_size=100, # 10000 Replay buffer warm-up\n",
        "    epsilon_start=1.0,\n",
        "    epsilon_end=0.01,\n",
        "    epsilon_decay=500000,      # Decay epsilon over a large number of steps\n",
        "    tau=1e-3,\n",
        "    device=device,\n",
        "    dueling=True # activate dueling dqn\n",
        ")\n",
        "\n",
        "num_timesteps = 10000 #1_000_000\n",
        "obs = env.reset()\n",
        "\n",
        "# Variables to track rewards and performance\n",
        "episode_rewards = []\n",
        "current_rewards = np.zeros(env.num_envs, dtype=np.float32)\n",
        "\n",
        "# We'll use a tqdm progress bar for convenience\n",
        "for timestep in trange(num_timesteps, desc=\"Training steps\"):\n",
        "    # Select actions for each environment in the vectorized set\n",
        "    actions = []\n",
        "    for i in range(env.num_envs):\n",
        "        actions.append(agent.select_action(obs[i]))\n",
        "    actions = np.array(actions)\n",
        "\n",
        "    # Step through the vectorized environment\n",
        "    next_obs, rewards, dones, infos = env.step(actions)\n",
        "\n",
        "    shaped_rewards = []\n",
        "    for i in range(env.num_envs):\n",
        "        shaped_reward = reward_shaping(rewards[i], obs[i], next_obs[i])\n",
        "        shaped_rewards.append(shaped_reward)\n",
        "        agent.store_transition(obs[i], actions[i], shaped_reward, next_obs[i], dones[i])\n",
        "\n",
        "    # Perform a training step of the agent\n",
        "    agent.train_step()\n",
        "\n",
        "    # Update observation\n",
        "    obs = next_obs\n",
        "\n",
        "    # Accumulate rewards for each environment\n",
        "    current_rewards += shaped_rewards\n",
        "\n",
        "    # When an episode finishes in any environment, reset that environment and record rewards\n",
        "    for i, done in enumerate(dones):\n",
        "        if done:\n",
        "            episode_rewards.append(current_rewards[i])\n",
        "            current_rewards[i] = 0.0\n",
        "\n",
        "print(\"\\nTraining finished!\")\n",
        "\n",
        "# After training, you could save the model if desired\n",
        "torch.save(agent.q_network.state_dict(), \"dqn_lunarlander.pth\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BY_HuedOoISR"
      },
      "source": [
        "## Evaluate the agent\n",
        "- Now that our Lunar Lander agent is trained, we need to **check its performance**.\n",
        "\n",
        "**Note**: When you evaluate your agent, you should not use your training environment but create an evaluation environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRpno0glsADy"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gymnasium\n",
        "\n",
        "# Create a new environment for evaluation\n",
        "eval_env = gymnasium.make(\"LunarLander-v2\")\n",
        "num_eval_episodes = 10\n",
        "all_episode_rewards = []\n",
        "\n",
        "for _ in range(num_eval_episodes):\n",
        "    obs, info = eval_env.reset()\n",
        "    done = False\n",
        "    truncated = False\n",
        "    episode_reward = 0.0\n",
        "\n",
        "    while not (done or truncated):\n",
        "        action = agent.select_action(obs)\n",
        "        obs, reward, done, truncated, info = eval_env.step(action)\n",
        "        episode_reward += reward\n",
        "\n",
        "    all_episode_rewards.append(episode_reward)\n",
        "\n",
        "mean_reward = np.mean(all_episode_rewards)\n",
        "std_reward = np.std(all_episode_rewards)\n",
        "\n",
        "print(f\"Mean Reward over {num_eval_episodes} episodes: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
        "\n",
        "# ---------------------------\n",
        "# Plotting\n",
        "# ---------------------------\n",
        "\n",
        "# Plot the training episode rewards\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(episode_rewards, label='Episode Rewards')\n",
        "plt.title('Learning Curve (Episode Rewards Over Time)')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Total Reward')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot a smoothed version of the training curve using a moving average\n",
        "window_size = 50  # Adjust this window as needed\n",
        "if len(episode_rewards) > window_size:\n",
        "    smooth_rewards = np.convolve(episode_rewards, np.ones(window_size)/window_size, mode='valid')\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(smooth_rewards, label=f'Moving Average (window={window_size})', color='orange')\n",
        "    plt.title('Smoothed Learning Curve')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Smoothed Total Reward')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Plot the distribution of evaluation episode rewards\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.hist(all_episode_rewards, bins=10, alpha=0.7, label='Evaluation Rewards')\n",
        "plt.axvline(x=mean_reward, color='r', linestyle='dashed', linewidth=2, label=f'Mean Reward = {mean_reward:.2f}')\n",
        "plt.title('Distribution of Evaluation Episode Rewards')\n",
        "plt.xlabel('Total Reward')\n",
        "plt.ylabel('Count')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9exOLHYN_0e"
      },
      "source": [
        "# Create Video of the result\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yormgXARODPR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from gymnasium.wrappers import RecordVideo\n",
        "\n",
        "# Create a directory for the videos if it doesn't exist\n",
        "if not os.path.exists('./videos'):\n",
        "    os.makedirs('./videos')\n",
        "\n",
        "# Create a new evaluation environment with video recording\n",
        "eval_env = gym.make(\"LunarLander-v2\", render_mode=\"rgb_array\")\n",
        "eval_env = RecordVideo(eval_env, video_folder='./videos', episode_trigger=lambda episode_id: True)\n",
        "\n",
        "obs, info = eval_env.reset()\n",
        "done = False\n",
        "truncated = False\n",
        "\n",
        "while not (done or truncated):\n",
        "    # Select an action using the trained agent (no epsilon-greedy during evaluation)\n",
        "    action = agent.select_action(obs)\n",
        "    obs, reward, done, truncated, info = eval_env.step(action)\n",
        "\n",
        "eval_env.close()\n",
        "print(\"Video recorded! Check the './videos' folder for the output.\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "QAN7B0_HCVZC",
        "BqPKw3jt_pG5"
      ],
      "gpuType": "A100",
      "machine_shape": "hm",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "ed7f8024e43d3b8f5ca3c5e1a8151ab4d136b3ecee1e3fd59e0766ccc55e1b10"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
