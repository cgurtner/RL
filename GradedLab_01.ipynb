{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b4a38be-33e3-4b29-a2a2-4177c7b75153",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f23a253-c438-4503-8443-86ec9ff07ce4",
   "metadata": {},
   "source": [
    "# TASK 1 - Monte Carlo with a random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47e561e4-5a51-4dc2-bbf3-40b7b404be82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated State-Value Function (V):\n",
      "[[0.00497334 0.00432366 0.01007191 0.00332477]\n",
      " [0.00717027 0.         0.02241027 0.        ]\n",
      " [0.01829068 0.05515168 0.09674542 0.        ]\n",
      " [0.         0.13376857 0.3961039  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Frozen Lake environment\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
    "\n",
    "# Parameters\n",
    "n_episodes = 5000  # Number of episodes\n",
    "gamma = 0.9        # Discount factor\n",
    "\n",
    "# Initialize value table to zeros\n",
    "V = np.zeros(env.observation_space.n)  # State-value function\n",
    "returns_sum = np.zeros(env.observation_space.n)\n",
    "returns_count = np.zeros(env.observation_space.n)\n",
    "\n",
    "# Function to generate an episode following a random policy\n",
    "def generate_episode(env):\n",
    "    episode = []\n",
    "    state, _ = env.reset()  # Only take the state, ignore additional info\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = env.action_space.sample()  # Random action\n",
    "        next_state, reward, done, _, *_ = env.step(action)  # Extract only necessary info\n",
    "        episode.append((state, action, reward))\n",
    "        state = next_state\n",
    "    return episode\n",
    "\n",
    "# Monte Carlo with First-Visit approach\n",
    "for episode in range(n_episodes):\n",
    "    episode_data = generate_episode(env)\n",
    "    visited_states = set()  # Track first visits to each state\n",
    "    G = 0\n",
    "    \n",
    "    # Calculate returns and update state-value function\n",
    "    for t in reversed(range(len(episode_data))):\n",
    "        state, action, reward = episode_data[t]\n",
    "        G = gamma * G + reward  # Compute return\n",
    "\n",
    "        # First-Visit MC: update only if state hasn't been visited in this episode\n",
    "        if state not in visited_states:\n",
    "            visited_states.add(state)\n",
    "            returns_sum[state] += G\n",
    "            returns_count[state] += 1\n",
    "            V[state] = returns_sum[state] / returns_count[state]\n",
    "\n",
    "# Display learned value function\n",
    "print(\"Estimated State-Value Function (V):\")\n",
    "print(V.reshape((int(np.sqrt(env.observation_space.n)), -1)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79c57df-c978-48f9-a76a-207e0751785c",
   "metadata": {},
   "source": [
    "# TASK 2 - Incremental Monte Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ac26bbe-be82-4f8c-9f22-1c47f56c8ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated State-Value Function (V):\n",
      "[[1.33863730e-29 1.64760536e-18 6.45602498e-10 2.24356279e-17]\n",
      " [3.41484586e-13 0.00000000e+00 1.26014023e-04 0.00000000e+00]\n",
      " [1.27773292e-06 4.38329478e-03 2.12302652e-02 0.00000000e+00]\n",
      " [0.00000000e+00 1.69336862e-02 2.68238318e-01 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Frozen Lake environment\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
    "\n",
    "# Parameters\n",
    "n_episodes = 5000        # Number of episodes\n",
    "gamma = 0.9              # Discount factor\n",
    "epsilon = 1.0            # Initial exploration rate\n",
    "min_epsilon = 0.1        # Minimum exploration rate\n",
    "epsilon_decay = 0.995    # Decay rate for epsilon\n",
    "alpha = 0.1              # Learning rate for incremental updates\n",
    "\n",
    "# Initialize value table to zeros\n",
    "V = np.zeros(env.observation_space.n)  # State-value function\n",
    "\n",
    "# Function to choose action using ε-Greedy policy\n",
    "def choose_action(state):\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        return env.action_space.sample()  # Explore: random action\n",
    "    else:\n",
    "        # Exploit: Choose action with highest state value (for Frozen Lake, random choice is sufficient)\n",
    "        return env.action_space.sample()  # Actions are random under Monte Carlo without Q-values\n",
    "\n",
    "# Function to generate an episode following ε-Greedy policy\n",
    "def generate_episode():\n",
    "    episode = []\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = choose_action(state)\n",
    "        next_state, reward, done, _, *_ = env.step(action)\n",
    "        episode.append((state, action, reward))\n",
    "        state = next_state\n",
    "    return episode\n",
    "\n",
    "# Incremental Monte Carlo\n",
    "for episode in range(n_episodes):\n",
    "    episode_data = generate_episode()\n",
    "    G = 0\n",
    "    \n",
    "    # Loop through the episode in reverse for incremental updates\n",
    "    for t in reversed(range(len(episode_data))):\n",
    "        state, action, reward = episode_data[t]\n",
    "        G = gamma * G + reward  # Compute return\n",
    "        \n",
    "        # Incremental update of state-value function\n",
    "        V[state] += alpha * (G - V[state])\n",
    "\n",
    "    # Decay ε to reduce exploration over time\n",
    "    epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "\n",
    "# Display learned value function\n",
    "print(\"Estimated State-Value Function (V):\")\n",
    "print(V.reshape((int(np.sqrt(env.observation_space.n)), -1)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efd1d25-3c46-4752-ae15-96bc9af84be6",
   "metadata": {},
   "source": [
    "# TASK 3 - Q-Learning Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "133ffe96-2c16-442f-8741-28b55ef8872b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned Q-Table:\n",
      "[[0.0480898  0.04769605 0.04912855 0.04742664]\n",
      " [0.03517695 0.0347992  0.03896944 0.03997207]\n",
      " [0.04108799 0.04104216 0.04170399 0.04021703]\n",
      " [0.03063678 0.02490211 0.02457026 0.03341926]\n",
      " [0.05757836 0.04816095 0.06772309 0.03771445]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.05630977 0.04687008 0.04044596 0.0216042 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.07329008 0.09572027 0.09492424 0.12856263]\n",
      " [0.14161543 0.19515744 0.19742256 0.16016432]\n",
      " [0.15775859 0.15681431 0.16307182 0.12599444]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.19117669 0.25327488 0.30759128 0.30780088]\n",
      " [0.41920573 0.49072502 0.50110053 0.47353171]\n",
      " [0.         0.         0.         0.        ]]\n",
      "Derived Policy (best action for each state):\n",
      "[[2 3 2 3]\n",
      " [2 0 0 0]\n",
      " [3 2 2 0]\n",
      " [0 3 2 0]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Frozen Lake environment\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
    "\n",
    "# Parameters\n",
    "n_episodes = 5000       # Number of episodes\n",
    "gamma = 0.9             # Discount factor\n",
    "alpha = 0.1             # Learning rate\n",
    "epsilon = 1.0           # Initial exploration rate\n",
    "min_epsilon = 0.1       # Minimum exploration rate\n",
    "epsilon_decay = 0.995   # Decay rate for epsilon\n",
    "\n",
    "# Initialize Q-table to zeros\n",
    "Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "# Function to choose action using ε-Greedy policy based on Q-values\n",
    "def choose_action(state):\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        return env.action_space.sample()  # Explore: random action\n",
    "    else:\n",
    "        return np.argmax(Q[state])  # Exploit: action with max Q-value for the state\n",
    "\n",
    "# Q-Learning algorithm\n",
    "for episode in range(n_episodes):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # Choose action with ε-Greedy policy\n",
    "        action = choose_action(state)\n",
    "        next_state, reward, done, _, *_ = env.step(action)\n",
    "\n",
    "        # Q-Learning update\n",
    "        best_next_action = np.argmax(Q[next_state])  # Best action at next state\n",
    "        Q[state, action] += alpha * (reward + gamma * Q[next_state, best_next_action] - Q[state, action])\n",
    "\n",
    "        state = next_state  # Move to the next state\n",
    "\n",
    "    # Decay ε to reduce exploration over time\n",
    "    epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "\n",
    "# Display learned Q-table\n",
    "print(\"Learned Q-Table:\")\n",
    "print(Q)\n",
    "\n",
    "# Optional: Derive and display the policy from Q-table\n",
    "policy = np.argmax(Q, axis=1)\n",
    "print(\"Derived Policy (best action for each state):\")\n",
    "print(policy.reshape((int(np.sqrt(env.observation_space.n)), -1)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
