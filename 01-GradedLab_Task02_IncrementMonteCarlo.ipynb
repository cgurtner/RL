{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GradedLab 1 \n",
    "## Task 1: Monte Carlo with random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from typing import NamedTuple\n",
    "from gymnasium.envs.toy_text.frozen_lake import generate_random_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Params(NamedTuple):\n",
    "    total_episodes: int \n",
    "    map_size: int\n",
    "    seed: int\n",
    "    is_slippery: bool\n",
    "    proba_frozen: float\n",
    "    gamma: float\n",
    "    epsilon: float\n",
    "    learning_rate: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots\n",
    "def plot_trajectory_lengths(trajectory_lengths):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(trajectory_lengths)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Trajectory Length (Steps)\")\n",
    "    plt.title(\"Length of Trajectories per Episode\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_learning_curve(episode_returns):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    cumulative_returns = np.cumsum(episode_returns)\n",
    "    plt.plot(cumulative_returns)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Cumulative Reward\")\n",
    "    plt.title(\"Learning Curve (Cumulative Reward over Episodes)\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_value_function(V, params: Params):\n",
    "    size = params.map_size\n",
    "    values = np.zeros((size, size))\n",
    "    for state in range(len(V)):\n",
    "        row, col = divmod(state, size)\n",
    "        values[row, col] = V[state]\n",
    "    \n",
    "    plt.figure(figsize=(7, 6))  \n",
    "    plt.imshow(values, cmap='viridis', interpolation='nearest') \n",
    "    plt.colorbar(label=\"State-Value (V)\")\n",
    "    plt.title(\"State-Value Function (Monte Carlo First-Visit)\")\n",
    "    plt.xlabel(\"State (Columns)\")\n",
    "    plt.ylabel(\"State (Rows)\")\n",
    "    \n",
    "    for i in range(size):\n",
    "        for j in range(size):\n",
    "            plt.text(j, i, f\"{values[i, j]:.2f}\", ha='center', va='center', color=\"white\" if values[i, j] < 0.3 else \"black\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incremental Monte Carlo with ε-Greedy\n",
    "def incremental_monte_carlo(params: Params):\n",
    "    random_map = generate_random_map(size=params.map_size, p=params.proba_frozen)\n",
    "    env = gym.make(\"FrozenLake-v1\", desc=random_map, is_slippery=params.is_slippery)\n",
    "    env.reset(seed=params.seed)\n",
    "\n",
    "    # Initialize value array\n",
    "    V = np.zeros(env.observation_space.n)\n",
    "    episode_lengths = []\n",
    "    episode_returns = []\n",
    "\n",
    "    for episode in range(params.total_episodes):\n",
    "        episode_data = generate_episode_with_epsilon_greedy(env, V, params.epsilon)\n",
    "        episode_lengths.append(len(episode_data))\n",
    "\n",
    "        G = 0  # cumulative reward\n",
    "        visited_states = set()  # Track visited states within this episode\n",
    "\n",
    "        for t in reversed(range(len(episode_data))):\n",
    "            state, action, reward = episode_data[t]\n",
    "            G = reward + params.gamma * G  # Discounted reward\n",
    "\n",
    "            # First-Visit check\n",
    "            if state not in visited_states:\n",
    "                visited_states.add(state)\n",
    "                # Incremental update using learning rate and G\n",
    "                V[state] = V[state] + params.learning_rate * (G - V[state])\n",
    "\n",
    "        episode_returns.append(G)\n",
    "\n",
    "    return V, episode_lengths, episode_returns\n",
    "\n",
    "\n",
    "# Generate an episode with ε-Greedy policy\n",
    "def generate_episode_with_epsilon_greedy(env, V, epsilon):\n",
    "    episode = []\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        # ε-Greedy action selection\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = env.action_space.sample()  # explore\n",
    "        else:\n",
    "            q_values = [V[state] for state in range(env.observation_space.n)]\n",
    "            action = np.argmax(q_values)  # exploit\n",
    "\n",
    "        next_state, reward, done, _, *_ = env.step(action)\n",
    "        episode.append((state, action, reward))\n",
    "        state = next_state\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5x5 configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_5x5 = Params(total_episodes=5000, map_size=5, seed=123, is_slippery=False, proba_frozen=0.9, gamma=0.95, epsilon=0.1, learning_rate=0.8)\n",
    "V_5x5_incremental, trajectory_lengths_5x5_incremental, episode_returns_5x5_incremental = incremental_monte_carlo(params_5x5)\n",
    "\n",
    "# Plot results\n",
    "plot_trajectory_lengths(trajectory_lengths_5x5_incremental)\n",
    "plot_learning_curve(episode_returns_5x5_incremental)\n",
    "plot_value_function(V_5x5_incremental, params_5x5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11x11 configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_11x11 = Params(total_episodes=20000, map_size=11, seed=123, is_slippery=False, proba_frozen=0.9)\n",
    "V_11x11, trajectory_lengths_11x11, episode_returns_11x11 = monte_carlo_first_visit(params_11x11)\n",
    "\n",
    "plot_trajectory_lengths(trajectory_lengths_11x11)\n",
    "plot_learning_curve(episode_returns_11x11)\n",
    "plot_value_function(V_11x11, params_11x11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>total_episodes=5000</strong><br><br>\n",
    "A total of 5,000 episodes allows the agent to explore a sufficient number of trajectories in the environment. Since we are using a Monte Carlo approach, which relies on sampling complete episodes, a larger number of episodes helps the agent gather enough data to estimate the state-value function more accurately, even when actions are selected randomly. This number provides a balance between computational efficiency and obtaining a reliable estimate of the state values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>is_slippery=False</strong><br><br>\n",
    "Setting `is_slippery=False` removes the stochastic element of movement in Frozen Lake, making it a deterministic environment. This means that when the agent takes an action, it moves in the intended direction without the risk of slipping to a random adjacent tile. A deterministic environment simplifies the learning process and allows for a clearer evaluation of the Monte Carlo method without added randomness in state transitions. This makes it easier to observe the effects of the random policy on the value estimation alone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>proba_frozen=0.9</strong><br><br>\n",
    "A high probability (90%) of frozen tiles means that the majority of the grid will be traversable, with only a few tiles potentially being holes. This makes it more likely that the agent will reach the goal in most episodes, allowing it to gather more data on successful trajectories. This setting supports the goal of value estimation without the agent frequently falling into holes, which would add noise to the returns and make it harder to observe consistent trends in the value function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Comparison total_episodes=20000</strong><br><br>\n",
    "In the plot with 20,000 episodes, the trajectory lengths show a wider spread and higher variability in the number of steps per episode. This increased variability is expected with a larger state space (11x11 grid) and higher number of episodes, as the agent continues to explore randomly over a much larger area.\n",
    "\n",
    "The longer episode count allows the agent to sample more diverse trajectories, which helps in capturing the complex layout of the larger grid. However, even with 20,000 episodes, the random policy does not necessarily yield efficient trajectories, as shown by the frequent spikes in trajectory lengths.\n",
    "\n",
    "The cumulative reward plot for 20,000 episodes shows a slow, step-like progression, indicating infrequent successful completions of episodes with positive rewards. The increase is more gradual and sparse compared to the 5,000-episode run, likely due to the increased difficulty in reaching the goal in the larger grid environment.\n",
    "\n",
    "The state-value heatmap after 20,000 episodes displays a clear gradient, with values increasing as states get closer to the goal in the bottom-right corner. Compared to the 5,000-episode case, the additional episodes have allowed the agent to refine its estimates of state values, particularly in areas close to the goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choice of Incremental MC Implementation (First-Visit / Multiple-Visits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " For this assignment, we chose the First-Visit Monte Carlo approach because it provides an unbiased estimation of the value function, as each state's first visit is treated as an independent sample. In stochastic environments like Frozen Lake, this helps ensure that our estimates are not skewed by repeated visits to the same state within a single trajectory, which can lead to redundant data in the case of Multiple-Visit. This also aligns with standard reinforcement learning practices where First-Visit Monte Carlo is often preferred due to its unbiased nature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of Hyperparameter Values Chosen for Each Method, and Selection Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the random policy, there are no hyperparameters to tune. The random policy simply selects actions uniformly from the available action space without any dependence on learned values or exploration rates.\n",
    "\n",
    "In a random policy, the agent’s actions do not depend on learned information or environmental feedback. This lack of dependency means we have no parameters to adjust, as the policy is inherently stochastic and fixed by design. This approach provides a baseline to assess performance without additional complexity from exploration-exploitation balancing or value estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is the Agent Able to Estimate the Value Function Correctly?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent is able to estimate an approximate value function for the environment. In the state-value function heatmap, we observe that states closer to the goal (bottom right corner) have higher values, while states further away generally have lower values. This indicates that the agent has correctly captured the general structure of the environment, where proximity to the goal state corresponds to higher returns. However, since the policy is random, the values are not optimal and reflect the average returns under a random policy rather than a strategic or goal-oriented policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is the Agent Able to Learn the Right Policy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, the agent does not learn the optimal policy with the random action selection approach. The trajectory length plot shows high variability in the number of steps required to complete episodes, indicating inconsistency in reaching the goal efficiently. Furthermore, the cumulative reward plot increases slowly and steadily, showing that while the agent accumulates some rewards over time, it does not improve in a way that would signify learning an efficient policy. To learn the right policy, the agent would need a mechanism to exploit higher-value states by following a more directed strategy, rather than acting randomly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
